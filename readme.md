# Indexing Shortest Paths Between Vertices and Vertex Groups in Weighted Knowledge Graphs

The introduction of these files are as follows. 


# Datasets

There are six datasets: Musae, Twitch, Github, Amazon, DBLP, Reddit (zipped together; should be unzipped first). 

There are sie files for each dataset. For example, the Amazon dataset contains the following files: 
1. amazon_edges.txt: the edges, as well as random and Jacard edge weights.
2. amazon_vertex_names.txt: the vertices as well as vertex names.
3. amazon_graph_Jacard_ec.binary: the binary file of an adjacency list of the Amazon graph with Jacard weights. This file will be read by the following codes.
4. amazon_graph_random_ec.binary: the binary file of an adjacency list of the Amazon graph with random weights. This file will be read by the following codes.
5. amazon_MDC_query_list.binary: the randomly generated query list for the indexing algorithms in the main experiments (each query is a pair of dummy and non-dummy vertices). This file will be read by the following codes.
6. amazon_query_list.binary: the randomly generated query list for the GST and MDC algorithms in the main experiments (each query is a set of dummy vertices, respresenting the set of vertex groups in the GST and MDC problems). This file will be read by the following codes.





# C++ codes 

The C++ source codes are in <b>PGST.cpp</b>. 

It is recommended to fold all the regions of codes for easy reading (by pressing Ctrl+M+O in Visual Studio). 

Running these codes requires some header files (e.g. #include <graph_hash_of_mixed_weighted/graph_hash_of_mixed_weighted.h>; see cppheader_202*****.zip) and the Boost library: https://www.boost.org/ (e.g. #include <boost/random.hpp>) . 

We use the above datasets to generate binary graph files for the experiments. The reason for generating binary files is that it is much faster to read binary files than to read raw data files. We use the function "produce_binary_graph_files_for_experiments" in the codes to generate all binary graph files for the experiments in our paper. For example, the binary file "amazon_read_graph_one_edge_weight_548552.bin" is generated by the above function, and records the full amazon graph where all edge costs are 1.

After generating binary graph files, we use the function "generate_binary_PLL_indexes" to generate hub labels that record shortest paths in graphs. For example, the label file "PLL_binary_amazon_one_edge_weight_548552.txt" is generated by the above function, and records the shortest paths in the full amazon graph where all edge costs are 1. Notably, each label file for a DBLP graph consumes dozens of GB memory and storage. So, make sure there is enough space for generating and saving these files. These labels are required for our experiments. 

After making the header files; binary graph files; and hub labels ready, <b>all the experiments in our paper can be conducted by running the function "experiments()"</b>, Make sure there is enough memory (at least 200 GB memory for hub labels and other consumption).

To read these C++ codes in detail, it is recommended to start from "experiments()", and then go to "experiment_element". More detailed codes in other regions can then be traced, such as the codes of proposed three algorithms below "/*proposed algorithms*/"


